# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TLcz712YSZupAC9f_su6jAt6VCb2gnpw
"""

!pip install -U bitsandbytes
!pip install -U accelerate
!pip install -U transformers

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import os

# Ensure GPU is available and set the device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Using device: {device}")

# Set Hugging Face token (replace with your own token)
access_token = ""
os.environ["HF_TOKEN"] = access_token

# Install required packages (quietly to avoid clutter)
!pip install -q accelerate bitsandbytes transformers

# Efficient models for Colab T4 GPU
model_a_name = "microsoft/phi-2"             # Generator model (optimized, small)
model_b_name = "mistralai/Mistral-7B-v0.1"   # Refiner model (4-bit quantized)

# 4-bit Quantization + CPU Offloading for better memory management
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# Load Model A (Generator) with quantization
model_a = AutoModelForCausalLM.from_pretrained(
    model_a_name,
    device_map="auto",
    quantization_config=bnb_config
)
tokenizer_a = AutoTokenizer.from_pretrained(model_a_name, token=access_token)

# Load Model B (Refiner) with quantization
model_b = AutoModelForCausalLM.from_pretrained(
    model_b_name,
    device_map="auto",
    quantization_config=bnb_config
)
tokenizer_b = AutoTokenizer.from_pretrained(model_b_name, token=access_token)

# Clear GPU cache to prevent memory fragmentation
torch.cuda.empty_cache()

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# Ensure GPU usage if available
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Using device: {device}")

# Text Generation Function (with max_new_tokens)
def generate_text(model, tokenizer, prompt, max_new_tokens=300):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.8,  # Keeps variety in responses
        top_p=0.9,        # Nucleus sampling for more balanced output
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# AI Collaboration Loop (Balanced Consensus Approach)
def collaborative_loop(topic, rounds=5):
    print(f"ü§ù Starting AI Collaboration on: {topic}")

    content = f"Write a comprehensive and detailed article about {topic}."
    consensus_threshold = 0.2  # Lower threshold = harder to reach consensus

    for i in range(rounds):
        print(f"\nüîÑ Round {i + 1}")

        # Model A generates content
        print("ü§ñ Model A (Generator) is writing...")
        output_a = generate_text(model_a, tokenizer_a, content, max_new_tokens=400)
        print(f"\nüìú Model A Output:\n{output_a}")

        # Model B reviews and refines (Focused feedback)
        feedback_prompt = f"""
        Carefully review the following article. Identify only 1-2 specific areas where the article can be improved or corrected.
        Focus on factual accuracy, coherence, or significant missing points.
        If the article is already comprehensive and requires no further changes, say exactly: "No major changes needed."

        Article:
        {output_a}

        Provide focused feedback:
        """
        print("\nüßê Model B (Refiner) is reviewing...")
        feedback = generate_text(model_b, tokenizer_b, feedback_prompt, max_new_tokens=300)
        print(f"\nüí¨ Model B Feedback:\n{feedback}")

        # Improve content based on feedback
        content = f"""
        Revise the article by incorporating the following feedback.
        Ensure that the changes only address the specific feedback and do not alter already accurate sections.

        Article:
        {output_a}

        Feedback:
        {feedback}

        Provide the revised article:
        """

        # Consensus Check (with Reduced Optimism)
        if "no major changes needed" in feedback.lower() and torch.rand(1).item() < consensus_threshold:
            print("\n‚úÖ Consensus Reached!\n")
            print(output_a)
            break
    else:
        print("\nüö® Max Rounds Reached. Final Content:\n", output_a)

if __name__ == "__main__":
    topic = input("Enter a topic for AI collaboration: ")
    collaborative_loop(topic)